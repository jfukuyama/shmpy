Using TensorFlow backend.
WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-15
OMP: Info #156: KMP_AFFINITY: 16 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 8 cores/pkg x 1 threads/core (16 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 2 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 3 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 4 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 8 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 10 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 11 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 12 
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 1 core 0 
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 1 core 2 
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 1 core 3 
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 1 core 4 
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 1 core 8 
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 10 
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 1 core 11 
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 12 
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 6904 thread 0 bound to OS proc set 0
WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

OMP: Info #250: KMP_AFFINITY: pid 6904 tid 8531 thread 1 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 11756 thread 2 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 11757 thread 3 bound to OS proc set 9
2019-10-03 23:06:07.791332: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-10-03 23:06:07.801605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3200225000 Hz
2019-10-03 23:06:07.801684: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe989612950 executing computations on platform Host. Devices:
2019-10-03 23:06:07.801695: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-03 23:06:07.801770: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2019-10-03 23:06:16.789738: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 11764 thread 4 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12298 thread 7 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12297 thread 6 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12296 thread 5 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12300 thread 8 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12302 thread 10 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12301 thread 9 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12303 thread 11 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12304 thread 12 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12305 thread 13 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12306 thread 14 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12307 thread 15 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12308 thread 16 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12309 thread 17 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12310 thread 18 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12311 thread 19 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 11765 thread 20 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12314 thread 22 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12313 thread 21 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12315 thread 23 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12316 thread 24 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12317 thread 25 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12318 thread 26 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12319 thread 27 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12320 thread 28 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12321 thread 29 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12322 thread 30 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12323 thread 31 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12325 thread 33 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12324 thread 32 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12327 thread 35 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 6904 tid 12326 thread 34 bound to OS proc set 1
__________________________________________________________________________________________
Layer (type)                            Output Shape                        Param #       
==========================================================================================
time_distributed_1 (TimeDistributed)    (None, None, 301, 1, 64)            4672          
__________________________________________________________________________________________
time_distributed_2 (TimeDistributed)    (None, None, 301, 64, 1)            0             
__________________________________________________________________________________________
time_distributed_3 (TimeDistributed)    (None, None, 150, 32, 1)            0             
__________________________________________________________________________________________
time_distributed_4 (TimeDistributed)    (None, None, 143, 29, 32)           1056          
__________________________________________________________________________________________
time_distributed_5 (TimeDistributed)    (None, None, 71, 14, 32)            0             
__________________________________________________________________________________________
time_distributed_6 (TimeDistributed)    (None, None, 31808)                 0             
__________________________________________________________________________________________
time_distributed_7 (TimeDistributed)    (None, None, 16)                    508944        
__________________________________________________________________________________________
simple_rnn_1 (SimpleRNN)                (None, 9)                           234           
==========================================================================================
Total params: 514,906
Trainable params: 514,906
Non-trainable params: 0
__________________________________________________________________________________________
None
Epoch 1/200
 - 171s - loss: 1.5355 - val_loss: 1.2348
Epoch 2/200
 - 166s - loss: 1.3044 - val_loss: 1.0731
Epoch 3/200
 - 171s - loss: 1.1525 - val_loss: 0.9728
Epoch 4/200
 - 173s - loss: 1.0454 - val_loss: 0.9568
Epoch 5/200
 - 172s - loss: 0.9880 - val_loss: 0.9950
Epoch 6/200
 - 178s - loss: 1.0154 - val_loss: 1.0464
Epoch 7/200
 - 174s - loss: 1.0934 - val_loss: 1.0782
Epoch 8/200
 - 173s - loss: 1.1496 - val_loss: 1.0821
Epoch 9/200
 - 175s - loss: 1.1260 - val_loss: 1.0645
Epoch 10/200
 - 170s - loss: 1.1804 - val_loss: 1.0345
Epoch 11/200
 - 173s - loss: 1.1106 - val_loss: 1.0019
Epoch 12/200
 - 174s - loss: 1.0638 - val_loss: 0.9747
Epoch 13/200
 - 174s - loss: 1.0084 - val_loss: 0.9584
Epoch 14/200
 - 175s - loss: 1.0574 - val_loss: 0.9540
Epoch 15/200
 - 172s - loss: 1.0623 - val_loss: 0.9595
Epoch 16/200
 - 176s - loss: 1.0139 - val_loss: 0.9703
Epoch 17/200
 - 174s - loss: 1.0351 - val_loss: 0.9812
Epoch 18/200
 - 174s - loss: 1.0630 - val_loss: 0.9884
Epoch 19/200
 - 173s - loss: 1.0311 - val_loss: 0.9895
Epoch 20/200
 - 172s - loss: 1.0640 - val_loss: 0.9853
Epoch 21/200
 - 173s - loss: 1.0709 - val_loss: 0.9773
Epoch 22/200
 - 172s - loss: 1.0020 - val_loss: 0.9680
Epoch 23/200
 - 168s - loss: 1.0798 - val_loss: 0.9602
Epoch 24/200
 - 174s - loss: 1.0442 - val_loss: 0.9552
Epoch 25/200
 - 171s - loss: 1.0468 - val_loss: 0.9538
Epoch 26/200
 - 173s - loss: 1.0170 - val_loss: 0.9557
Epoch 27/200
 - 173s - loss: 0.9966 - val_loss: 0.9593
Epoch 28/200
 - 178s - loss: 1.0251 - val_loss: 0.9631
Epoch 29/200
 - 172s - loss: 1.0637 - val_loss: 0.9654
Epoch 30/200
 - 172s - loss: 1.0826 - val_loss: 0.9656
Epoch 31/200
 - 169s - loss: 1.1196 - val_loss: 0.9639
Epoch 32/200
 - 171s - loss: 1.0348 - val_loss: 0.9609
Epoch 33/200
 - 178s - loss: 1.0045 - val_loss: 0.9577
Epoch 34/200
 - 171s - loss: 1.0258 - val_loss: 0.9551
Epoch 35/200
 - 170s - loss: 1.0369 - val_loss: 0.9538
Epoch 36/200
 - 173s - loss: 1.0182 - val_loss: 0.9538
Epoch 37/200
 - 174s - loss: 1.0032 - val_loss: 0.9548
Epoch 38/200
 - 171s - loss: 1.0322 - val_loss: 0.9562
Epoch 39/200
 - 175s - loss: 0.9904 - val_loss: 0.9572
Epoch 40/200
 - 172s - loss: 0.9852 - val_loss: 0.9575
Epoch 41/200
 - 174s - loss: 0.9779 - val_loss: 0.9571
Epoch 42/200
 - 169s - loss: 1.0494 - val_loss: 0.9561
Epoch 43/200
 - 174s - loss: 1.0293 - val_loss: 0.9549
Epoch 44/200
 - 175s - loss: 1.0003 - val_loss: 0.9539
Epoch 45/200
 - 175s - loss: 1.0037 - val_loss: 0.9535
Epoch 46/200
 - 170s - loss: 0.9674 - val_loss: 0.9536
Epoch 47/200
 - 171s - loss: 1.0101 - val_loss: 0.9541
Epoch 48/200
 - 176s - loss: 1.0208 - val_loss: 0.9546
Epoch 49/200
 - 175s - loss: 1.0027 - val_loss: 0.9550
Epoch 50/200
 - 172s - loss: 1.0583 - val_loss: 0.9551
Epoch 51/200
 - 172s - loss: 1.0501 - val_loss: 0.9549
Epoch 52/200
 - 169s - loss: 0.9834 - val_loss: 0.9542
Epoch 53/200
 - 170s - loss: 1.0355 - val_loss: 0.9536
Epoch 54/200
 - 172s - loss: 1.0204 - val_loss: 0.9533
Epoch 55/200
 - 170s - loss: 1.0385 - val_loss: 0.9533
Epoch 56/200
 - 174s - loss: 0.9828 - val_loss: 0.9534
Epoch 57/200
 - 170s - loss: 0.9525 - val_loss: 0.9537
Epoch 58/200
 - 171s - loss: 0.9823 - val_loss: 0.9540
Epoch 59/200
 - 172s - loss: 1.0302 - val_loss: 0.9541
Epoch 60/200
 - 173s - loss: 1.0529 - val_loss: 0.9539
Epoch 61/200
 - 175s - loss: 0.9745 - val_loss: 0.9536
Epoch 62/200
 - 175s - loss: 1.0303 - val_loss: 0.9533
Epoch 63/200
 - 173s - loss: 1.0152 - val_loss: 0.9531
Epoch 64/200
 - 167s - loss: 0.9991 - val_loss: 0.9531
Epoch 65/200
 - 173s - loss: 0.9839 - val_loss: 0.9531
Epoch 66/200
 - 176s - loss: 1.0540 - val_loss: 0.9531
Epoch 67/200
 - 174s - loss: 0.9924 - val_loss: 0.9532
Epoch 68/200
 - 173s - loss: 0.9966 - val_loss: 0.9532
Epoch 69/200
 - 173s - loss: 1.0102 - val_loss: 0.9532
Epoch 70/200
 - 172s - loss: 0.9941 - val_loss: 0.9531
Epoch 71/200
 - 171s - loss: 0.9924 - val_loss: 0.9530
Epoch 72/200
 - 175s - loss: 1.0354 - val_loss: 0.9529
Epoch 73/200
 - 175s - loss: 0.9664 - val_loss: 0.9529
Epoch 74/200
 - 174s - loss: 1.0307 - val_loss: 0.9528
Epoch 75/200
 - 173s - loss: 0.9861 - val_loss: 0.9528
Epoch 76/200
 - 173s - loss: 1.0526 - val_loss: 0.9528
Epoch 77/200
 - 174s - loss: 1.0565 - val_loss: 0.9528
Epoch 78/200
 - 170s - loss: 0.9832 - val_loss: 0.9528
Epoch 79/200
 - 175s - loss: 0.9946 - val_loss: 0.9527
Epoch 80/200
 - 172s - loss: 0.9967 - val_loss: 0.9527
Epoch 81/200
 - 176s - loss: 1.0571 - val_loss: 0.9527
Epoch 82/200
 - 172s - loss: 1.0353 - val_loss: 0.9527
Epoch 83/200
 - 178s - loss: 0.9745 - val_loss: 0.9527
Epoch 84/200
 - 171s - loss: 1.0153 - val_loss: 0.9528
Epoch 85/200
 - 173s - loss: 1.0402 - val_loss: 0.9527
Epoch 86/200
 - 172s - loss: 0.9546 - val_loss: 0.9527
Epoch 87/200
 - 173s - loss: 1.0277 - val_loss: 0.9526
Epoch 88/200
 - 171s - loss: 1.0415 - val_loss: 0.9525
Epoch 89/200
 - 171s - loss: 1.0175 - val_loss: 0.9525
Epoch 90/200
 - 173s - loss: 0.9994 - val_loss: 0.9525
Epoch 91/200
 - 171s - loss: 1.0186 - val_loss: 0.9525
Epoch 92/200
 - 169s - loss: 1.0079 - val_loss: 0.9525
Epoch 93/200
 - 176s - loss: 1.0049 - val_loss: 0.9525
Epoch 94/200
 - 170s - loss: 1.0164 - val_loss: 0.9524
Epoch 95/200
 - 175s - loss: 0.9942 - val_loss: 0.9524
Epoch 96/200
 - 170s - loss: 1.0117 - val_loss: 0.9524
Epoch 97/200
 - 172s - loss: 1.0424 - val_loss: 0.9523
Epoch 98/200
 - 175s - loss: 1.0314 - val_loss: 0.9523
Epoch 99/200
 - 175s - loss: 1.0374 - val_loss: 0.9523
Epoch 100/200
 - 174s - loss: 1.0844 - val_loss: 0.9523
Epoch 101/200
 - 173s - loss: 0.9696 - val_loss: 0.9523
Epoch 102/200
 - 175s - loss: 1.0140 - val_loss: 0.9523
Epoch 103/200
 - 175s - loss: 0.9966 - val_loss: 0.9523
Epoch 104/200
 - 167s - loss: 1.0138 - val_loss: 0.9522
Epoch 105/200
 - 170s - loss: 1.0303 - val_loss: 0.9521
Epoch 106/200
 - 174s - loss: 0.9860 - val_loss: 0.9521
Epoch 107/200
 - 175s - loss: 1.0068 - val_loss: 0.9521
Epoch 108/200
 - 171s - loss: 1.0042 - val_loss: 0.9520
Epoch 109/200
 - 175s - loss: 1.0501 - val_loss: 0.9520
Epoch 110/200
 - 175s - loss: 1.0092 - val_loss: 0.9520
Epoch 111/200
 - 176s - loss: 1.0290 - val_loss: 0.9520
Epoch 112/200
 - 169s - loss: 1.0041 - val_loss: 0.9519
Epoch 113/200
 - 172s - loss: 1.0508 - val_loss: 0.9519
Epoch 114/200
 - 168s - loss: 1.0454 - val_loss: 0.9519
Epoch 115/200
 - 172s - loss: 1.0551 - val_loss: 0.9519
Epoch 116/200
 - 175s - loss: 0.9773 - val_loss: 0.9518
Epoch 117/200
 - 172s - loss: 1.0150 - val_loss: 0.9518
Epoch 118/200
 - 171s - loss: 1.0374 - val_loss: 0.9518
Epoch 119/200
 - 176s - loss: 0.9852 - val_loss: 0.9518
Epoch 120/200
 - 172s - loss: 1.0341 - val_loss: 0.9517
Epoch 121/200
 - 174s - loss: 1.0338 - val_loss: 0.9517
Epoch 122/200
 - 178s - loss: 1.0199 - val_loss: 0.9517
Epoch 123/200
 - 170s - loss: 1.0390 - val_loss: 0.9517
Epoch 124/200
 - 171s - loss: 1.0328 - val_loss: 0.9517
Epoch 125/200
 - 174s - loss: 1.0188 - val_loss: 0.9517
Epoch 126/200
 - 173s - loss: 1.0375 - val_loss: 0.9517
Epoch 127/200
 - 174s - loss: 1.0163 - val_loss: 0.9517
Epoch 128/200
 - 177s - loss: 1.0517 - val_loss: 0.9518
Epoch 129/200
 - 171s - loss: 1.0283 - val_loss: 0.9517
Epoch 130/200
 - 174s - loss: 0.9866 - val_loss: 0.9516
Epoch 131/200
 - 172s - loss: 1.0335 - val_loss: 0.9515
Epoch 132/200
 - 172s - loss: 1.0103 - val_loss: 0.9514
Epoch 133/200
 - 168s - loss: 0.9915 - val_loss: 0.9514
Epoch 134/200
 - 174s - loss: 1.0154 - val_loss: 0.9515
Epoch 135/200
 - 173s - loss: 0.9831 - val_loss: 0.9517
Epoch 136/200
 - 173s - loss: 1.0806 - val_loss: 0.9517
Epoch 137/200
 - 172s - loss: 1.0571 - val_loss: 0.9515
Epoch 138/200
 - 174s - loss: 1.0114 - val_loss: 0.9513
Epoch 139/200
 - 173s - loss: 1.0202 - val_loss: 0.9512
Epoch 140/200
 - 176s - loss: 1.0153 - val_loss: 0.9512
Epoch 141/200
 - 175s - loss: 1.0120 - val_loss: 0.9513
Epoch 142/200
 - 171s - loss: 0.9938 - val_loss: 0.9513
Epoch 143/200
 - 171s - loss: 1.0319 - val_loss: 0.9512
Epoch 144/200
 - 173s - loss: 1.0278 - val_loss: 0.9512
Epoch 145/200
 - 172s - loss: 1.0023 - val_loss: 0.9511
Epoch 146/200
 - 173s - loss: 0.9896 - val_loss: 0.9510
Epoch 147/200
 - 170s - loss: 1.0093 - val_loss: 0.9510
Epoch 148/200
 - 174s - loss: 1.0731 - val_loss: 0.9510
Epoch 149/200
 - 168s - loss: 1.0407 - val_loss: 0.9510
Epoch 150/200
 - 173s - loss: 1.0010 - val_loss: 0.9510
Epoch 151/200
 - 176s - loss: 1.0205 - val_loss: 0.9510
Epoch 152/200
 - 166s - loss: 1.0210 - val_loss: 0.9510
Epoch 153/200
 - 176s - loss: 1.0064 - val_loss: 0.9509
Epoch 154/200
 - 177s - loss: 1.0071 - val_loss: 0.9508
Epoch 155/200
 - 173s - loss: 0.9760 - val_loss: 0.9508
Epoch 156/200
 - 177s - loss: 1.0241 - val_loss: 0.9509
Epoch 157/200
 - 175s - loss: 1.0229 - val_loss: 0.9510
Epoch 158/200
 - 171s - loss: 1.0190 - val_loss: 0.9510
Epoch 159/200
 - 171s - loss: 0.9931 - val_loss: 0.9509
Epoch 160/200
 - 175s - loss: 1.0039 - val_loss: 0.9507
Epoch 161/200
 - 174s - loss: 1.0072 - val_loss: 0.9506
Epoch 162/200
 - 173s - loss: 0.9835 - val_loss: 0.9506
Epoch 163/200
 - 172s - loss: 0.9612 - val_loss: 0.9507
Epoch 164/200
 - 175s - loss: 0.9580 - val_loss: 0.9508
Epoch 165/200
 - 176s - loss: 1.0066 - val_loss: 0.9508
Epoch 166/200
 - 171s - loss: 1.0289 - val_loss: 0.9507
Epoch 167/200
 - 175s - loss: 1.0171 - val_loss: 0.9505
Epoch 168/200
 - 176s - loss: 0.9847 - val_loss: 0.9505
Epoch 169/200
 - 170s - loss: 1.0103 - val_loss: 0.9505
Epoch 170/200
 - 171s - loss: 0.9916 - val_loss: 0.9505
Epoch 171/200
 - 175s - loss: 0.9557 - val_loss: 0.9504
Epoch 172/200
 - 171s - loss: 0.9734 - val_loss: 0.9504
Epoch 173/200
 - 174s - loss: 0.9992 - val_loss: 0.9503
Epoch 174/200
 - 168s - loss: 0.9961 - val_loss: 0.9503
Epoch 175/200
 - 169s - loss: 0.9691 - val_loss: 0.9504
Epoch 176/200
 - 171s - loss: 1.0033 - val_loss: 0.9505
Epoch 177/200
 - 171s - loss: 0.9868 - val_loss: 0.9506
Epoch 178/200
 - 178s - loss: 1.0074 - val_loss: 0.9504
Epoch 179/200
 - 172s - loss: 1.0064 - val_loss: 0.9502
Epoch 180/200
 - 176s - loss: 1.0056 - val_loss: 0.9501
Epoch 181/200
 - 171s - loss: 1.0445 - val_loss: 0.9501
Epoch 182/200
 - 170s - loss: 1.0191 - val_loss: 0.9502
Epoch 183/200
 - 174s - loss: 1.0494 - val_loss: 0.9503
Epoch 184/200
 - 177s - loss: 1.0038 - val_loss: 0.9503
Epoch 185/200
 - 171s - loss: 1.0299 - val_loss: 0.9503
Epoch 186/200
 - 172s - loss: 0.9986 - val_loss: 0.9502
Epoch 187/200
 - 170s - loss: 1.0084 - val_loss: 0.9500
Epoch 188/200
 - 169s - loss: 1.0142 - val_loss: 0.9499
Epoch 189/200
 - 173s - loss: 0.9805 - val_loss: 0.9500
Epoch 190/200
 - 169s - loss: 0.9947 - val_loss: 0.9503
Epoch 191/200
 - 171s - loss: 0.9811 - val_loss: 0.9506
Epoch 192/200
 - 177s - loss: 1.0134 - val_loss: 0.9504
Epoch 193/200
 - 171s - loss: 1.0819 - val_loss: 0.9501
Epoch 194/200
 - 173s - loss: 0.9864 - val_loss: 0.9498
Epoch 195/200
 - 172s - loss: 1.0170 - val_loss: 0.9497
Epoch 196/200
 - 176s - loss: 1.0044 - val_loss: 0.9498
Epoch 197/200
 - 172s - loss: 1.0205 - val_loss: 0.9499
Epoch 198/200
 - 172s - loss: 0.9760 - val_loss: 0.9498
Epoch 199/200
 - 171s - loss: 1.0377 - val_loss: 0.9497
Epoch 200/200
 - 171s - loss: 1.0420 - val_loss: 0.9496
